{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using ZhipuAI GLM-4 though OpenAI SDK and Ecosystem\n",
    "\n",
    "**This tutorial is available in English and is attached below the Chinese explanation**\n",
    "\n",
    "æœ¬èŠ‚ä»£ç å°†å¸¦é¢†å¤§å®¶ä½¿ç”¨`OpenAI`çš„ SDK å’Œæ”¯æŒçš„æ¡†æž¶è°ƒç”¨ `GLM-4` æ¨¡åž‹\n",
    "ä½ å°†åœ¨æœ¬ç« èŠ‚å­¦ä¹ åˆ°å¦‚ä½•ä½¿ç”¨ Langchain çš„ `ChatOpenAI` ç±»æ¥è°ƒç”¨ ZhipuAI çš„ GLM-4 æ¨¡åž‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ `OpenAI SDK` ç±»æ¥å®žçŽ°è°ƒç”¨GLM-4æ¨¡åž‹ã€‚\n",
    "\n",
    "This notebook section will walk you through calling `GLM-4` models using the `OpenAI` SDK and supporting frameworks.\n",
    "In this section you will learn how to call ZhipuAI's GLM-4 models using Langchain's `ChatOpenAI` class, and how to use the `OpenAI SDK` class to implement calling GLM-4 models.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a626e20197e21c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Use OpenAI SDK\n",
    "\n",
    "é¦–å…ˆï¼Œéœ€è¦å¯¼å…¥ `OpenAI` SDKï¼ŒåŒæ—¶ï¼Œä¿®æ”¹ `OpenAI` çš„ `base_url` ä¸º `https://open.bigmodel.cn/api/paas/v4/` \n",
    "ä»¥ä¾¿è°ƒç”¨ `GLM-4` æ¨¡åž‹ã€‚å®Œæˆæ›´æ¢åŽï¼Œä»…éœ€å¡«å†™ Key å°±èƒ½ä½¿ç”¨ `GLM-4` æ¨¡åž‹ã€‚\n",
    "\n",
    "First, you need to import the `OpenAI` SDK and change the `OpenAI` `base_url` to `https://open.bigmodel.cn/api/paas/v4/` in order to call the `GLM-4` model. \n",
    "in order to call the `GLM-4` model. Once you have done this, you can use the `GLM-4` model by simply filling in the Key.\n",
    "``python"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b58e7836f5b885b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=\"your zhipuai key\",\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\")"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T03:38:45.899372Z",
     "start_time": "2024-04-23T03:38:45.633362Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Simple use of OpenAI SDK to call GLM-4\n",
    "\n",
    "æˆ‘å°†å¸¦é¢†å¤§å®¶å®Œæˆæœ€åŸºç¡€çš„å¯¹è¯è°ƒç”¨ã€‚\n",
    "è¯·æ³¨æ„ï¼š\n",
    "- `temperature` å‚æ•°çš„åŒºé—´ä¸º (0,1)ã€‚\n",
    "- `do_sample = False (temperature = 0)` åœ¨ OpenAI è°ƒç”¨ä¸­å¹¶ä¸é€‚ç”¨ã€‚\n",
    "-  å¼‚æ­¥æ“ä½œä¸é€‚ç”¨ã€‚\n",
    "\n",
    "å…¶ä»–ä¾‹å­(ä¾‹å¦‚å·¥å…·è°ƒç”¨)ï¼Œä¹Ÿå‡å¯ä»¥æŒ‰ç…§OpenAIçš„æ–¹å¼è¿›è¡Œè°ƒç”¨ã€‚\n",
    "\n",
    "\n",
    "I will lead everyone to complete the most basic dialogue calls.\n",
    "Please note:\n",
    "\n",
    "- The interval of the `temperature` parameter is (0,1).\n",
    "- `do_sample=False (temperature=0)` is not applicable in OpenAI calls.\n",
    "- Asynchronous operations are not applicable.\n",
    "\n",
    "Other examples (such as tool calls) can also be called in the same way as OpenAI."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11e3e2b3eddb8819"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ChatCompletion(id='8595685587681558870', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", role='assistant', function_call=None, tool_calls=None))], created=1713843527, model='glm-4', object=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=9, total_tokens=24), request_id='8595685587681558870')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"tell me a joke\"\n",
    "        }\n",
    "    ],\n",
    "    top_p=0.7,\n",
    "    temperature=0.9,\n",
    "    stream=False,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T03:38:47.447042Z",
     "start_time": "2024-04-23T03:38:45.900671Z"
    }
   },
   "id": "46b558c16e324725",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e96faeabc4754841"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Using Langchain's ChatOpenAI class\n",
    "\n",
    "Langchain çš„ `ChatOpenAI` ç±»æ˜¯å¯¹ `OpenAI` SDK çš„å°è£…ï¼Œå¯ä»¥æ›´æ–¹ä¾¿è°ƒç”¨ã€‚è¿™é‡Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `ChatOpenAI` ç±»æ¥è°ƒç”¨ `GLM-4` æ¨¡åž‹ã€‚\n",
    "\n",
    "Langchain's `ChatOpenAI` class is a wrapper around the `OpenAI` SDK, making it easier to call. Here we show how to use the `ChatOpenAI` class to call the `GLM-4` model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba894923a1806fc2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: tell me a joke\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'question': 'tell me a joke',\n 'chat_history': [HumanMessage(content='tell me a joke'),\n  AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything! ðŸ˜„\")],\n 'text': \"Why don't scientists trust atoms?\\n\\nBecause they make up everything! ðŸ˜„\"}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=\"your zhipuai api key\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "conversation.invoke({\"question\": \"tell me a joke\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T03:42:00.468839Z",
     "start_time": "2024-04-23T03:41:59.394044Z"
    }
   },
   "id": "95ca98869dcb246a",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Calling GLM-4 with LangChain AgentExecutor\n",
    "åŒæ—¶ï¼ŒGLM-4ä¹Ÿèƒ½å¾ˆå¥½çš„æŽ¥å…¥åˆ° LangChain çš„ AgentExecutor ä¸­ï¼Œè¿™é‡Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `AgentExecutor` æ¥è°ƒç”¨ `GLM-4` æ¨¡åž‹ã€‚\n",
    "\n",
    "GLM-4 also plugs nicely into LangChain's AgentExecutor, which shows how to call a `GLM-4` model using `AgentExecutor`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c6c560191f25ec5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mTo answer this question, I should search for information about LangChain. This sounds like a technical term, so I might need to use a search engine that can provide comprehensive and accurate results.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain\"\n",
      "Observation\u001B[0m\u001B[36;1m\u001B[1;3m[{'url': 'https://docs.smith.langchain.com/cookbook/testing-examples/agent_steps', 'content': 'We will also store expected steps in our dataset to demonstrate the sequence of actions the agent is expected to\\ntake in order to resolve the query.\\n Define agent\\u200b\\nThe main components of an agentic program are:\\nIn this example, we will create an agent with access to a DuckDuckGo search client (for informational search) and a mock tool to check a user\\'s calendar for a given date.\\n Once you\\'ve configured a custom evaluator for this type of evaluation, it\\'s easy to apply other techniques using off-the-shelf evaluators like LangChain\\'s TrajectoryEvalChain, which can instruct an LLM to grade the efficacy of the agent\\'s actions. Since our dataset has multiple output keys, we have to instruct the run_on_dataset function on which key to use as the ground truth for the QA evaluator by setting reference_key=\"reference\" below.\\n By the end of this guide, you\\'ll have a better sense of how to apply an evaluator to more complex inputs like an agent\\'s trajectory.\\n'}, {'url': 'https://python.langchain.com/docs/modules/agents/', 'content': \"If we've set up LangSmith, we'll see a trace that let's us inspect the input and output to each step in the sequence: https://smith.langchain.com/public/04110122-01a8-413c-8cd0-b4df6eefa4b7/r\\nDefine the runtime\\u200b\\nSo this is just the first step - now we need to write a runtime for this.\\n In pseudocode, this looks roughly like:\\nWhile this may seem simple, there are several complexities this runtime handles for you, including:\\nOther types of agent runtimes\\u200b\\nThe AgentExecutor class is the main agent runtime supported by LangChain.\\n We will import two last utility functions: a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, and a component for converting the output message into an agent action/agent finish.\\n For a full list of built-in toolkits, see the toolkits integrations section\\nAgentExecutor\\u200b\\nThe agent executor is the runtime for an agent.\\n Let's pass in a simple question and empty intermediate steps and see what it returns:\\nWe can see that it responds with an AgentAction to take (it's actually an AgentActionMessageLog - a subclass of AgentAction which also tracks the full message log).\\n\"}]\u001B[0m\u001B[32;1m\u001B[1;3mBased on my initial search, LangChain appears to be a framework or platform that is used to build and evaluate AI agents. It provides tools and components that allow developers to create agents that can perform tasks and make decisions. LangChain also includes features for testing and evaluating the performance of these agents.\n",
      "\n",
      "However, my understanding is still quite limited, and I might need to perform additional searches or explore specific resources to provide a more detailed and accurate answer.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain explained\"\n",
      "Observation\u001B[0m\u001B[36;1m\u001B[1;3m[{'url': 'https://nanonets.com/blog/langchain/', 'content': \"Here's how you can achieve this using LCEL with a RunnableBranch, which is initialized with a list of (condition, runnable) pairs and a default runnable:\\nThe final chain is then constructed using various components, such as a topic classifier, prompt branch, and an output parser, to determine the flow based on the topic of the input:\\nThis approach exemplifies the flexibility and power of LangChain in handling complex queries and routing them appropriately based on the input.\\n Here's an example of how it's used in Python:\\nAnother tool is the CharacterTextSplitter, which splits text based on a specified character and includes controls for chunk size and overlap:\\nThe HTMLHeaderTextSplitter is designed to split HTML content based on header tags, retaining the semantic structure:\\nA more complex manipulation can be achieved by combining HTMLHeaderTextSplitter with another splitter, like the Pipelined Splitter:\\nLangChain also offers specific splitters for different programming languages, like the Python Code Splitter and the JavaScript Code Splitter:\\n Behind the scenes, a googleapi resource is created using the following methods:\\nThe toolkit offers various tools that can be used within an agent, including:\\nTo use these tools within an agent, you can initialize the agent as follows:\\nHere are a couple of examples of how these tools can be used:\\nThese examples demonstrate the capabilities of LangChain's Gmail toolkit within an agent, enabling you to interact with Gmail programmatically.\\n This involves two steps:\\nLet's start by adding a memory placeholder in the prompt:\\nNow, create a list to track the chat history:\\nIn the agent creation step, we'll include the memory as well:\\nNow, when running the agent, make sure to update the chat history:\\n The process begins with installing the necessary packages and setting up the environment:\\nThe schema for tagging is defined, specifying the properties and their expected types:\\nExamples of running the tagging chain with different inputs show the model's ability to interpret sentiments, languages, and aggressiveness:\\n\"}, {'url': 'https://blog.langchain.dev/planning-agents/', 'content': 'LLMCompiler\\nðŸ”— Python Link\\nThe LLMCompiler, by Kim, et. al., is an agent architecture designed to further increase the speed of task execution beyond the plan-and-execute and ReWOO agents described above, and even beyond OpenAIâ€™s parallel tool calling.\\n As an example, given the user query \"What are the stats for the quarterbacks of the super bowl contenders this year\", the planner may generate the following plan:\\nNotice how the planner can reference previous outputs using syntax like #E2 . Once execution is completed, the agent is called again with a re-planning prompt, letting it decide whether to finish with a response or whether to generate a follow-up plan (if the first plan didnâ€™t have the desired effect).\\n If your application requires multiple tool invocations or API calls, these types of approaches can reduce the time it takes to return a final result and help you save costs by reducing the frequency of calls to more powerful LLMs.\\n The ReAct agent is a great prototypical design for this, as it prompts the language model using a repeated thought, act, observation loop:\\nA typical ReAct-style agent trajectory.\\n'}]\u001B[0m\u001B[32;1m\u001B[1;3mBased on the additional search results, LangChain seems to be a powerful framework for building and manipulating AI agents. It provides various components and tools that enable developers to handle complex queries and routing, interact with different APIs, and create agents with memory and decision-making capabilities. LangChain includes features like topic classifiers, prompt branches, output parsers, and various splitters for different programming languages. It also supports the integration of memory and chat history, which allows agents to maintain context and improve interactions.\n",
      "\n",
      "LangChain appears to be highly flexible and can be used for a wide range of applications, from simple task execution to advanced decision-making and planning.\n",
      "\n",
      "Thought: I now have a better understanding of LangChain and can provide a more detailed answer.\n",
      "\n",
      "Final Answer: LangChain is a comprehensive framework for building and evaluating AI agents. It offers a wide range of components, tools, and features that enable developers to create sophisticated agents capable of handling complex tasks, making decisions, and interacting with various APIs. LangChain supports the manipulation of agent trajectories, memory integration, and advanced planning, making it a powerful platform for AI development.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'what is LangChain?',\n 'output': 'LangChain is a comprehensive framework for building and evaluating AI agents. It offers a wide range of components, tools, and features that enable developers to create sophisticated agents capable of handling complex tasks, making decisions, and interacting with various APIs. LangChain supports the manipulation of agent trajectories, memory integration, and advanced planning, making it a powerful platform for AI development.'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"your tavily api key\"\n",
    "tools = [TavilySearchResults(max_results=2)]\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "# Choose the LLM to use\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T03:39:14.321435Z",
     "start_time": "2024-04-23T03:38:47.990959Z"
    }
   },
   "id": "4db6cc6154c30560",
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
