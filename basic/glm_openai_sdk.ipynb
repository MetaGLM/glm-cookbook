{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using GLM though OpenAI SDK and Ecosystem\n",
    "\n",
    "This notebook section will walk you through calling `GLM-4.5` models using the `OpenAI` SDK and supporting frameworks.\n",
    "In this section you will learn how to call GLM-4.5 models using Langchain's `ChatOpenAI` class, and how to use the `OpenAI SDK` class to implement calling GLM-4 models.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a626e20197e21c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Use OpenAI SDK\n",
    "\n",
    "First, you need to import the `OpenAI` SDK and change the `OpenAI` `base_url` to `https://open.bigmodel.cn/api/paas/v4/` in order to call the `GLM-4` model. \n",
    "in order to call the `GLM-4` model. Once you have done this, you can use the `GLM-4` model by simply filling in the Key."
   ],
   "id": "c783c427ea4cf68e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T07:18:00.758105Z",
     "start_time": "2025-09-04T07:18:00.452713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"your api key of z.ai\",\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\")"
   ],
   "id": "aa53d9545b3945f5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Simple use of OpenAI SDK to call GLM-4.5\n",
    "\n",
    "Other examples (such as tool calls) can also be called in the same way as OpenAI."
   ],
   "id": "423408beb6e3bcda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T07:18:08.270799Z",
     "start_time": "2025-09-04T07:18:00.763611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"tell me a joke\"\n",
    "        }\n",
    "    ],\n",
    "    top_p=0.7,\n",
    "    temperature=0.9,\n",
    "    stream=False,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "response.choices[0].message.content"
   ],
   "id": "3f2e6a910fd50b43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! Here's a clean, punny one for you:\\n\\n**Why don't penguins fly?**  \\n*Because they're not tall enough to be pilots!*  \\n\\n*(...ba-dum-tss!)* ðŸ˜„\\n\\nWant another? Iâ€™ve got puns, dad jokes, or even a shaggy dog story if youâ€™re brave!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Using Langchain's ChatOpenAI class\n",
    "\n",
    "\n",
    "\n",
    "Langchain's `ChatOpenAI` class is a wrapper around the `OpenAI` SDK, making it easier to call. Here we show how to use the `ChatOpenAI` class to call the `GLM-4.5` model."
   ],
   "id": "393a7504ac638f90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T07:18:12.135985Z",
     "start_time": "2025-09-04T07:18:08.380209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4.5\",\n",
    "    openai_api_key=\"your api key of z.ai\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "conversation.invoke({\"question\": \"tell me a joke\"})"
   ],
   "id": "6388ba587cfe89ee",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cz/76xt997d6kl_rwv3_7mx_nm40000gn/T/ipykernel_59098/3774650906.py:28: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "/var/folders/cz/76xt997d6kl_rwv3_7mx_nm40000gn/T/ipykernel_59098/3774650906.py:29: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: tell me a joke\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'tell me a joke',\n",
       " 'chat_history': [HumanMessage(content='tell me a joke', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={})],\n",
       " 'text': \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Calling GLM-4.5 with LangChain AgentExecutor\n",
    "\n",
    "\n",
    "GLM-4 also plugs nicely into LangChain's AgentExecutor, which shows how to call a `GLM-4.5` model using `AgentExecutor`."
   ],
   "id": "424a2d612b28c1a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T07:22:50.990980Z",
     "start_time": "2025-09-04T07:19:50.390448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"your api keys of tavily\"\n",
    "tools = [TavilySearch(max_results=2)]\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "# Choose the LLM to use\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
   ],
   "id": "50cac302ee7ee72f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mThought: I need to understand what LangChain is. Since this is a technical concept that might have recent developments, I should search for a comprehensive definition and explanation.\n",
      "\n",
      "Action: tavily_search\n",
      "Action Input: \"what is LangChain\"\n",
      "Observation\u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'what is LangChain\"\\nObservation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.reddit.com/r/LangChain/comments/18xjizi/difference_between_answer_and_observation/', 'title': 'Difference between Answer and Observation? - LangChain - Reddit', 'content': \": r/LangChain : r/LangChain Image 1: r/LangChain icon Go to LangChain r/LangChain Image 2: r/LangChain iconr/LangChain LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. I am using a custom callback handler to do that, and I can stream the intermediate SQL query and everything, but not the 'Answer', or the 'Observation'. Comparing LangChain with other LLM frameworks *   Reddit reReddit: Top posts of January 3, 2024 * * * *   Reddit reReddit: Top posts of January 2024 * * * *   Reddit reReddit: Top posts of 2024 * * * *   Games  *   Other Games *   Animated Movies & Series *   About Reddit *   Best of Reddit Reddit, Inc.\", 'score': 0.79732054, 'raw_content': None}, {'url': 'https://www.langchain.com/langsmith', 'title': 'LangSmith - LangChain', 'content': 'LangSmith is a unified observability & evals platform where teams can debug, test, and monitor AI app performance â€” whether building with LangChain or not. Evaluate your app by saving production traces to datasets â€”\\xa0 then score performance with LLM-as-Judge evaluators. Yes, you can log traces to LangSmith using a standard OpenTelemetry client to access all LangSmith features, including tracing, running evals, and prompt engineering. How can LangSmith help with observability and evaluation? LangSmith traces contain the full information of all the inputs and outputs of each step of the application, giving users full visibility into their agent or LLM app behavior. LangSmith also allows users to instantly run evals to assess agent or LLM app performance â€” including LLM-as-Judge evaluators for auto-scoring and the ability to attach human feedback.', 'score': 0.76002824, 'raw_content': None}], 'response_time': 1.44, 'request_id': 'fe0ad7a7-0dbb-46e9-8109-2b6ff259f3c9'}\u001B[0m\u001B[32;1m\u001B[1;3mThought: The initial search provided some basic information about LangChain from a Reddit post, indicating it's an open-source framework for LLM applications. However, I need more comprehensive and authoritative information to provide a complete answer. I should search for the official LangChain documentation or a detailed overview from a reliable source.\n",
      "\n",
      "Action: tavily_search\n",
      "Action Input: \"LangChain official documentation overview\"\n",
      "Observation\u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'LangChain official documentation overview\"\\nObservation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://docs.langchain.com/', 'title': 'Overview - Docs by LangChain', 'content': 'Trusted by companies shaping the future of agents â€“ including Klarna, Replit, Elastic, and more â€“ LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. from langgraph.prebuilt import create_react_agent # Run the agent Or, to learn how to build an agent workflow with a customizable architecture, long-term memory, and other complex task handling, see the LangGraph basics tutorials. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. * LangGraph Platform â€” Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows.', 'score': 0.98526, 'raw_content': None}, {'url': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ðŸ¦œï¸   LangChain', 'content': 'LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle.', 'score': 0.98127, 'raw_content': None}], 'response_time': 1.56, 'request_id': 'd9392de5-1e51-4c6a-8001-a080accbbf8c'}\u001B[0m\u001B[32;1m\u001B[1;3m\u001B[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001B[32;1m\u001B[1;3mThought: I need to gather comprehensive information about LangChain from authoritative sources. The initial search provided a basic definition, but I should get more detailed information from the official documentation to provide a complete answer.\n",
      "\n",
      "Action: tavily_search\n",
      "Action Input: \"LangChain official documentation introduction and components\"\n",
      "Observation\u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'LangChain official documentation introduction and components\"\\nObservation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ðŸ¦œï¸   LangChain', 'content': 'LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle.', 'score': 0.65279555, 'raw_content': None}, {'url': 'https://js.langchain.com/docs/introduction/', 'title': 'Introduction | ðŸ¦œï¸   Langchain', 'content': \"**LangChain** is a framework for developing applications powered by large language models (LLMs). * **Development**: Build your applications using LangChain's open-source building blocks, components, and third-party integrations. * **`langchain`**: Chains, agents, and retrieval strategies that make up an application's cognitive architecture. * **LangGraph.js**: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Explore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. Read up on our Security best practices to make sure you're developing safely with LangChain. LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\", 'score': 0.6271529, 'raw_content': None}], 'response_time': 2.06, 'request_id': '0fbee0a6-1210-433d-ad3e-c4fad527b744'}\u001B[0m\u001B[32;1m\u001B[1;3mThought: I now have sufficient information from the official LangChain documentation to provide a comprehensive answer about what LangChain is. The search results clearly define LangChain as a framework for LLM-powered applications and provide details about its components and ecosystem.\n",
      "\n",
      "Final Answer: LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, from prototype to production. \n",
      "\n",
      "Key aspects of LangChain include:\n",
      "\n",
      "1. **Core Framework**: Provides building blocks, components, and third-party integrations for developing LLM applications\n",
      "2. **Cognitive Architecture**: Includes chains, agents, and retrieval strategies that form the application's reasoning capabilities\n",
      "3. **LangGraph**: A low-level orchestration framework for building stateful, multi-actor applications with LLMs by modeling steps as edges and nodes in a graph\n",
      "4. **Ecosystem Tools**:\n",
      "   - LangSmith: A unified observability and evaluation platform for debugging, testing, and monitoring AI applications\n",
      "   - LangGraph Platform: Deployment platform for scaling long-running, stateful workflows\n",
      "\n",
      "LangChain is trusted by major companies including Klarna, Replit, and Elastic. It can be used with various programming languages (Python, JavaScript) and integrates with both proprietary and open-source LLMs. The framework helps developers manage the entire lifecycle of LLM applications, from initial development through deployment and monitoring.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is LangChain?',\n",
       " 'output': \"LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, from prototype to production. \\n\\nKey aspects of LangChain include:\\n\\n1. **Core Framework**: Provides building blocks, components, and third-party integrations for developing LLM applications\\n2. **Cognitive Architecture**: Includes chains, agents, and retrieval strategies that form the application's reasoning capabilities\\n3. **LangGraph**: A low-level orchestration framework for building stateful, multi-actor applications with LLMs by modeling steps as edges and nodes in a graph\\n4. **Ecosystem Tools**:\\n   - LangSmith: A unified observability and evaluation platform for debugging, testing, and monitoring AI applications\\n   - LangGraph Platform: Deployment platform for scaling long-running, stateful workflows\\n\\nLangChain is trusted by major companies including Klarna, Replit, and Elastic. It can be used with various programming languages (Python, JavaScript) and integrates with both proprietary and open-source LLMs. The framework helps developers manage the entire lifecycle of LLM applications, from initial development through deployment and monitoring.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Using OpenAI SDK to call CogView-4\n",
    "\n",
    "We also support using the OpenAI interface to call the `CogView-4` model. Here we show how to use the `OpenAI SDK` class to call the `CogView-4` model. However, there are still many parameters that cannot be used."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e97ddd5dc94a835"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T07:23:15.879548Z",
     "start_time": "2025-09-04T07:22:56.301074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_cogview = client.images.generate(\n",
    "    model=\"cogview-4\",\n",
    "    prompt=\"A scene of a city inside a crystal bottle\"\n",
    ")\n",
    "response_cogview"
   ],
   "id": "23898a56fa3fac68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImagesResponse(created=1756970587, background=None, data=[Image(b64_json=None, revised_prompt=None, url='https://maas-watermark-prod.cn-wlcb.ufileos.com/1756970596375_watermark.png?UCloudPublicKey=TOKEN_75a9ae85-4f15-4045-940f-e94c0f82ae90&Signature=JzJpYkQXHzbcDnymM4p5780zgQI%3D&Expires=1757056996')], output_format=None, quality=None, size=None, usage=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T07:23:16.010973Z",
     "start_time": "2025-09-04T07:23:16.007693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(url=response_cogview.data[0].url))"
   ],
   "id": "ce5246231c23975a",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://maas-watermark-prod.cn-wlcb.ufileos.com/1756970596375_watermark.png?UCloudPublicKey=TOKEN_75a9ae85-4f15-4045-940f-e94c0f82ae90&Signature=JzJpYkQXHzbcDnymM4p5780zgQI%3D&Expires=1757056996\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. UsingOpenAI SDK to call Embedding Model\n",
    "\n",
    "\n",
    "We also support using the OpenAI interface to call the `Embedding` model. Here we show how to use the `OpenAI SDK` class to call the `Embedding` model.\n",
    "\n",
    "Please note that the input of the `Embedding` model does not currently support lists and can only be used as strings. Here, I only show the first 10 values."
   ],
   "id": "2dfde7f82248c8d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T07:23:16.187951Z",
     "start_time": "2025-09-04T07:23:16.063773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_embedding = client.embeddings.create(\n",
    "    model=\"embedding-3\",\n",
    "    input=\"hello\",\n",
    ")\n",
    "response_embedding.data[0].embedding[:10]"
   ],
   "id": "aa660dcd2bbad18d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0021513698,\n",
       " 0.0056886184,\n",
       " -0.04387432,\n",
       " 0.028451975,\n",
       " -0.025206413,\n",
       " 0.019141711,\n",
       " 0.0012881567,\n",
       " 0.020764494,\n",
       " -0.0116200615,\n",
       " 0.022091147]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
