{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Token Counting in ZhipuAI GLM API\n",
    "\n",
    "**This tutorial is available in English and is attached below the Chinese explanation**\n",
    "\n",
    "æ­¤ä»£ç å°†è®²è¿°åœ¨ GLM æ¨¡å‹ æ˜¯å¦‚ä½•è®¡ç®— Token çš„æ¶ˆè€—çš„ã€‚è¿™å°†èƒ½å¸®ä½ å¯¹åœ¨ API ä½¿ç”¨ä¸­çš„ Token è®¡ç®—äº§ç”Ÿæ›´æ¸…æ™°çš„è®¤è¯†ã€‚\n",
    "\n",
    "This cookbook will describe how Token consumption is calculated in the GLM model. This will help you have a clearer understanding of Token calculations in API usage."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c55dc5a09277b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Tokenizer and count\n",
    "\n",
    "ç”±äºæ²¡æœ‰ä»»ä½•çš„å…¬å¼€çš„ Zhipu AI tokenè®¡ç®—å·¥å…·ï¼Œå› æ­¤ï¼Œæˆ‘ä½¿ç”¨äº† [chatglm3-6b](https://huggingface.co/THUDM/chatglm3-6b) è¿™ä¸ªå¼€æºæ¨¡å‹çš„ tokenizerè¿›è¡ŒåŠ è½½ã€‚è¿™ç§è®¡ç®—æ–¹å¼ä»…èƒ½ä½œä¸ºå‚è€ƒï¼Œå°šä¸”ä¸èƒ½è®¤å®šæ˜¯æœ€ç»ˆçš„ API token è®¡ç®—æ–¹å¼ã€‚å…·ä½“çš„è®¡ä»·æ–¹å¼ä»¥å®˜æ–¹æ–‡æ¡£ä¸ºä¸»ã€‚\n",
    "\n",
    "Since there is no public Zhipu AI token calculation tool, I used the tokenizer of this open source model [chatglm3-6b](https://huggingface.co/THUDM/chatglm3-6b) to load it. This calculation method can only be used as a reference and cannot be considered as the final API token calculation method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8977deb25864ec9f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"chatglm3-6b\", trust_remote_code=True, encode_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:51:13.105834Z",
     "start_time": "2024-01-24T12:51:11.590578Z"
    }
   },
   "id": "e463ead175234bf7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "æˆ‘ä»¬ä¹¦å†™ä¸€ä¸ªPython ä»£ç ï¼Œåˆ†åˆ«æœ‰ä»¥ä¸‹ä¸¤ä¸ªåŠŸèƒ½\n",
    "\n",
    "1. `count_encode` : å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸º token å¹¶è®¡ç®— token æ¶ˆè€—ã€‚\n",
    "2. `decode`: è¾“å…¥ä¸€ä¸² token è¿›è¡Œè§£ç ï¼Œå¹¶å±•ç¤º è¯è¯­æ˜¯æ€ä¹ˆè¢«æ‹†å¼€çš„ã€‚\n",
    "\n",
    "we write a Python code with the following two functions:\n",
    "\n",
    "1. `count_encode`: Convert natural language into token and calculate token consumption.\n",
    "2. `decode`: Input a string of tokens to decode and show how the words are split."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7125d5fca775b004"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token: 7\n",
      "Token IDs: [64790, 64792, 985, 323, 260, 2254, 16948]\n",
      "Decode sentence: ['[gMASK]', 'sop', 'â–here', 'â–is', 'â–a', 'â–text', 'â–demo']\t"
     ]
    }
   ],
   "source": [
    "def count_encode(inputs: str = \"\"):\n",
    "    encoded_input = tokenizer.encode(inputs)\n",
    "    num_tokens = len(encoded_input)\n",
    "    return encoded_input, num_tokens\n",
    "\n",
    "\n",
    "def decode(inputs: list = []):\n",
    "    decode_sentence = tokenizer.convert_ids_to_tokens(inputs)\n",
    "    return decode_sentence\n",
    "\n",
    "\n",
    "text = \"here is a text demo\"\n",
    "encoded_input, num_tokens = count_encode(text)\n",
    "print(f\"Count Token: {num_tokens}\")\n",
    "print(f\"Token IDs: {encoded_input}\")\n",
    "decode_sentence = decode(encoded_input)\n",
    "print(f\"Decode sentence: {decode_sentence}\", end=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:51:13.111618Z",
     "start_time": "2024-01-24T12:51:13.107437Z"
    }
   },
   "id": "5b7ed321306f5849"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token calculation in conversation\n",
    "\n",
    "åˆšæ‰çš„ä»£ç å¹¶ä¸æ˜¯åœ¨çœŸæ­£å¯¹è¯ä¸­ token è®¡ç®—çš„ä½¿ç”¨é‡ï¼Œ åœ¨å¯¹è¯ä¸­ï¼Œç”±äºå­˜åœ¨æ¨¡å‹çš„ **å¯¹è¯æ¨¡æ¿** å’Œ **èŠå¤©å†å²**ï¼Œå¯¹è¯ä¸­çš„ token è®¡ç®—ä¼šæ˜æ˜¾å˜é«˜ã€‚\n",
    "\n",
    "æˆ‘å…ˆå±•ç¤ºäº†åœ¨å¯¹è¯åœºæ™¯ä¸­ï¼Œä¸€æ¡ message çš„å®é™…å¯¹åº”çš„tokenæ•°é‡ã€‚\n",
    "\n",
    "The code just now is not the usage of token calculation in the real conversation. In the conversation, due to the existence of the model's **conversation template** and **chat history**, the token calculation in the conversation will be significantly higher.\n",
    "\n",
    "I first showed the actual number of tokens corresponding to a message in the conversation scenario."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f78f812a8c1f0ea"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token: 11\n",
      "Token IDs: [64790, 64792, 64795, 265, 13, 985, 323, 260, 2254, 16948, 64796]\n",
      "['[gMASK]', 'sop', '<|user|>', 'â–â–', '<0x0A>', 'â–here', 'â–is', 'â–a', 'â–text', 'â–demo', '<|assistant|>']\t"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"here is a text demo\"})\n",
    "chat_inputs = tokenizer.apply_chat_template(messages,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            tokenize=True)\n",
    "num_tokens = len(chat_inputs)\n",
    "print(f\"Count Token: {num_tokens}\")\n",
    "print(f\"Token IDs: {chat_inputs}\")\n",
    "model_input_tokens = tokenizer.convert_ids_to_tokens(chat_inputs)\n",
    "print(model_input_tokens, end=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:51:13.140806Z",
     "start_time": "2024-01-24T12:51:13.110979Z"
    }
   },
   "id": "af6673d5c30f8841"
  },
  {
   "cell_type": "markdown",
   "source": [
    "å¯ä»¥çœ‹åˆ°ï¼Œç”±äº **å¯¹è¯æ¨¡æ¿** çš„å­˜åœ¨ï¼Œè¿™æ¬¡çš„ token è®¡ç®— å¤šå‡ºäº† 4 ä¸ª token æ•°é‡ï¼Œè¿™ä¸ªæ•°é‡å°±æ˜¯æ¨¡æ¿çš„å ç”¨ã€‚**è¿™éƒ¨åˆ† token ä¼šåœ¨ API è¿›è¡Œè®¡è´¹çš„è¿‡ç¨‹ä¸­è¢«æ‰£é™¤**ã€‚\n",
    "\n",
    "It can be seen that due to the existence of **dialogue template**, the token calculation this time has 4 more tokens, which is the occupation of the template. **This part of the token will be deducted during the API billing process**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d59444531c388c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculation of token consumption in actual conversations\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¼€å§‹æ¨¡æ‹Ÿåœ¨å®é™…çš„å¯¹è¯ä¸­ï¼Œtoken æ¶ˆè€—çš„æ–¹å¼ï¼Œé¦–å…ˆï¼Œä½ éœ€è¦å¡«å†™ API \n",
    "Now, we start to simulate the way token is consumed in an actual conversation. First, you need to fill in the API"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2b9d929eee22dfd"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = \"your api key\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:51:13.261739Z",
     "start_time": "2024-01-24T12:51:13.131065Z"
    }
   },
   "id": "1293b5a0561a0cfb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€æ®µå¸¦æœ‰å†å²è®°å½•çš„å¯¹è¯æ¥è®¡ç®—éšç€ å†å²è®°å½•çš„ä¸æ–­å¢åŠ ï¼Œ Tokençš„æ¶ˆè€—é€Ÿåº¦ã€‚\n",
    "æˆ‘ä¹¦å†™äº†ä¸€æ®µPythonè„šæœ¬ï¼Œåœ¨è¿™é‡Œï¼Œä½ å¯ä»¥å’Œæ¨¡å‹è¿›è¡Œè¿ç»­å¯¹è¯ï¼Œä»£ç å°†ä¼šè¿”å›éšç€å¯¹è¯çš„è¿›è¡Œï¼Œæ¯æ¬¡è¾“å…¥çš„ Token æ•°é‡ å’Œ è¾“å‡ºçš„ Tokenæ•°é‡ã€‚\n",
    "è¿™ä¸ªä»£ç æ¨¡æ‹Ÿäº†ä¸€ä¸ªç®€å•çš„å‘½ä»¤è¡Œäº¤äº’ç”»é¢ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥å’Œå¤§æ¨¡å‹è¿›è¡Œäº¤äº’ï¼ŒåŒæ—¶ï¼Œè¾“å‡ºä¼šè®¡ç®— ä½ çš„ è¾“å…¥ token æ¶ˆè€— å’Œ æ¨¡å‹çš„è¾“å‡ºtokenæ¶ˆè€—ã€‚\n",
    "è¯·æ³¨æ„ï¼Œ`è¾“å…¥æ¶ˆè€—` åœ¨ API æ¶ˆè€—çš„å€¼ï¼Œæ˜¯ ç”¨æˆ·å½“å‰è¾“å…¥å’Œå¯¹è¯å†å²çš„å…¨éƒ¨å†…å®¹ï¼Œå› æ­¤ï¼Œä½ å°†ä¼šåœ¨å¯¹è¯çš„è¿‡ç¨‹çœ‹åˆ° è¾“å…¥ä½¿ç”¨çš„ token é€æ¸å¢åŠ ã€‚\n",
    "\n",
    "Next, we use a conversation with historical records to calculate the Token consumption rate as the historical records continue to increase.\n",
    "I wrote a Python script, where you can have a continuous conversation with the model, and the code will return the number of Tokens input and the number of Tokens output each time as the conversation proceeds.\n",
    "\n",
    "This code simulates a simple command line interaction screen, where you can interact with the large model. At the same time, the output will calculate your input token consumption and the model's output token consumption.\n",
    "\n",
    "Please note that the value consumed by `input consumption` in the API is the entire content of the user's current input and conversation history. Therefore, you will see the token used for input gradually increase during the conversation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187bed9b6a2e5caa"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, max_tokens=8192):\n",
    "        self.client = ZhipuAI()\n",
    "        self.history = []\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"chatglm3-6b\",\n",
    "            trust_remote_code=True,\n",
    "            encode_special_tokens=True\n",
    "        )\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def calculate_tokens(self, inputs):\n",
    "        return len(tokenizer.apply_chat_template(\n",
    "            inputs,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True)\n",
    "        )\n",
    "\n",
    "    def chat(self):\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "            self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            print(f\"Human: {user_input}\")\n",
    "            user_tokens = self.calculate_tokens(self.history)\n",
    "            print(\"======================\")\n",
    "            print(f\"Human message with history tokens: {user_tokens}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"glm-4\",\n",
    "                messages=self.history,\n",
    "                top_p=0.7,\n",
    "                temperature=0.9,\n",
    "                stream=False,\n",
    "                max_tokens=self.max_tokens,\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content\n",
    "            print(f\"AI: {reply}\")\n",
    "            self.history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            reply_tokens = self.calculate_tokens(self.history[-2:])\n",
    "            print(\"======================\")\n",
    "            print(f\"AI response tokens: {reply_tokens}\", end=\"\\n**********************\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:51:13.267973Z",
     "start_time": "2024-01-24T12:51:13.265482Z"
    }
   },
   "id": "f03d04d64937fc37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "å¼€å§‹å¯¹è¯å§ï¼Œé€šè¿‡è¿™æ¬¡å¯¹è¯ï¼Œä½ åº”è¯¥ä¼šæ„è¯†åˆ°ç¼©çŸ­å†å²è®°å½•ï¼Œå¹¶å®šæœŸæ¸…ç©ºå†å²è®°å½•çš„é‡è¦æ€§ã€‚\n",
    "\n",
    "Start a conversationï¼That should help you realize the importance of shortening your history and clearing it regularly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92bda16255169230"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hello! what a nice day!\n",
      "======================\n",
      "Human message with history tokens: 13\n",
      "AI: Hello! Indeed, it looks like a beautiful day! Is there anything I can help you with today?\n",
      "======================\n",
      "AI response tokens: 37\n",
      "**********************\n",
      "Human: Are you good at joke?\n",
      "======================\n",
      "Human message with history tokens: 46\n",
      "AI: I can certainly share a joke or two! Here's a light-hearted one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Feel free to share a joke with me too, or let me know if you'd like to hear another one.\n",
      "======================\n",
      "AI response tokens: 80\n",
      "**********************\n",
      "Human: give me a example\n",
      "======================\n",
      "Human message with history tokens: 121\n",
      "AI: Sure! Here's another one:\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n",
      "\n",
      "And here's a classic math joke:\n",
      "\n",
      "What did the zero say to the eight?\n",
      "\n",
      "\"Nice belt!\" ğŸ™‚\n",
      "\n",
      "Let me know if you want more jokes or if there's anything else I can assist you with!\n",
      "======================\n",
      "AI response tokens: 95\n",
      "**********************\n",
      "Human: tell me some joke about school or university?\n",
      "======================\n",
      "Human message with history tokens: 218\n",
      "AI: Certainly! Here are a couple of jokes related to school and university:\n",
      "\n",
      "1. University Joke:\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything, including the grades!\n",
      "\n",
      "2. School Joke:\n",
      "Why was the math book sad?\n",
      "\n",
      "Because it had too many problems.\n",
      "\n",
      "And here's another one:\n",
      "\n",
      "Why did the student bring a sunflower to the computer class?\n",
      "\n",
      "To brighten up the byte!\n",
      "\n",
      "Hope these jokes bring a smile to your face. If you want more, just let me know!\n",
      "======================\n",
      "AI response tokens: 142\n",
      "**********************\n",
      "Human: Thank you!\n",
      "======================\n",
      "Human message with history tokens: 351\n",
      "AI: You're welcome! I'm glad I could bring a smile to your day. If you have any more questions or need assistance with anything else, feel free to ask. Have a great day!\n",
      "======================\n",
      "AI response tokens: 53\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "chat_session = ChatSession()\n",
    "chat_session.chat()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:52:19.532008Z",
     "start_time": "2024-01-24T12:51:13.267799Z"
    }
   },
   "id": "a651bbaad4bd2512"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
