{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Token Counting in ZhipuAI GLM API\n",
    "\n",
    "**This tutorial is available in English and is attached below the Chinese explanation**\n",
    "\n",
    "此代码将讲述在 GLM 模型 是如何计算 Token 的消耗的。这将能帮你对在 API 使用中的 Token 计算产生更清晰的认识。\n",
    "\n",
    "This cookbook will describe how Token consumption is calculated in the GLM model. This will help you have a clearer understanding of Token calculations in API usage."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c55dc5a09277b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Tokenizer and count\n",
    "\n",
    "由于没有任何的公开的 Zhipu AI token计算工具，因此，我使用了 [chatglm3-6b](https://huggingface.co/zai-org/GLM-4.5) 这个开源模型的 tokenizer进行加载。这种计算方式仅能作为参考，尚且不能认定是最终的 API token 计算方式。具体的计价方式以官方文档为主。\n",
    "\n",
    "Since there is no public Zhipu AI token calculation tool, I used the tokenizer of this open source model [chatglm3-6b](https://huggingface.co/zai-org/GLM-4.5) to load it. This calculation method can only be used as a reference and cannot be considered as the final API token calculation method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8977deb25864ec9f"
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.292515Z",
     "start_time": "2025-09-01T13:19:48.043898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zai-org/GLM-4.5\", trust_remote_code=True)"
   ],
   "id": "e463ead175234bf7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们书写一个Python 代码，分别有以下两个功能\n",
    "\n",
    "1. `count_encode` : 将自然语言转换为 token 并计算 token 消耗。\n",
    "2. `decode`: 输入一串 token 进行解码，并展示 词语是怎么被拆开的。\n",
    "\n",
    "we write a Python code with the following two functions:\n",
    "\n",
    "1. `count_encode`: Convert natural language into token and calculate token consumption.\n",
    "2. `decode`: Input a string of tokens to decode and show how the words are split."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7125d5fca775b004"
  },
  {
   "cell_type": "code",
   "source": [
    "def count_encode(inputs: str = \"\"):\n",
    "    encoded_input = tokenizer.encode(inputs)\n",
    "    num_tokens = len(encoded_input)\n",
    "    return encoded_input, num_tokens\n",
    "\n",
    "\n",
    "def decode(inputs: list = []):\n",
    "    decode_sentence = tokenizer.convert_ids_to_tokens(inputs)\n",
    "    return decode_sentence\n",
    "\n",
    "\n",
    "text = \"here is a text demo\"\n",
    "encoded_input, num_tokens = count_encode(text)\n",
    "print(f\"Count Token: {num_tokens}\")\n",
    "print(f\"Token IDs: {encoded_input}\")\n",
    "decode_sentence = decode(encoded_input)\n",
    "print(f\"Decode sentence: {decode_sentence}\", end=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.300417Z",
     "start_time": "2025-09-01T13:19:49.296460Z"
    }
   },
   "id": "5b7ed321306f5849",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token: 5\n",
      "Token IDs: [6739, 374, 264, 1467, 16644]\n",
      "Decode sentence: ['here', 'Ġis', 'Ġa', 'Ġtext', 'Ġdemo']\t"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token calculation in conversation\n",
    "\n",
    "刚才的代码并不是在真正对话中 token 计算的使用量， 在对话中，由于存在模型的 **对话模板** 和 **聊天历史**，对话中的 token 计算会明显变高。\n",
    "\n",
    "我先展示了在对话场景中，一条 message 的实际对应的token数量。\n",
    "\n",
    "The code just now is not the usage of token calculation in the real conversation. In the conversation, due to the existence of the model's **conversation template** and **chat history**, the token calculation in the conversation will be significantly higher.\n",
    "\n",
    "I first showed the actual number of tokens corresponding to a message in the conversation scenario."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f78f812a8c1f0ea"
  },
  {
   "cell_type": "code",
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"here is a text demo\"})\n",
    "chat_inputs = tokenizer.apply_chat_template(messages,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            tokenize=True)\n",
    "num_tokens = len(chat_inputs)\n",
    "print(f\"Count Token: {num_tokens}\")\n",
    "print(f\"Token IDs: {chat_inputs}\")\n",
    "model_input_tokens = tokenizer.convert_ids_to_tokens(chat_inputs)\n",
    "print(model_input_tokens, end=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.323615Z",
     "start_time": "2025-09-01T13:19:49.320136Z"
    }
   },
   "id": "af6673d5c30f8841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token: 10\n",
      "Token IDs: [151331, 151333, 151336, 198, 6739, 374, 264, 1467, 16644, 151337]\n",
      "['[gMASK]', '<sop>', '<|user|>', 'Ċ', 'here', 'Ġis', 'Ġa', 'Ġtext', 'Ġdemo', '<|assistant|>']\t"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，由于 **对话模板** 的存在，这次的 token 计算 多出了 4 个 token 数量，这个数量就是模板的占用。**这部分 token 会在 API 进行计费的过程中被扣除**。\n",
    "\n",
    "It can be seen that due to the existence of **dialogue template**, the token calculation this time has 4 more tokens, which is the occupation of the template. **This part of the token will be deducted during the API billing process**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d59444531c388c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculation of token consumption in actual conversations\n",
    "\n",
    "现在，我们开始模拟在实际的对话中，token 消耗的方式，首先，你需要填写 API \n",
    "Now, we start to simulate the way token is consumed in an actual conversation. First, you need to fill in the API"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2b9d929eee22dfd"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = \"your api key\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.473199Z",
     "start_time": "2025-09-01T13:19:49.465494Z"
    }
   },
   "id": "1293b5a0561a0cfb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "接着，我们使用一段带有历史记录的对话来计算随着 历史记录的不断增加， Token的消耗速度。\n",
    "我书写了一段Python脚本，在这里，你可以和模型进行连续对话，代码将会返回随着对话的进行，每次输入的 Token 数量 和 输出的 Token数量。\n",
    "这个代码模拟了一个简单的命令行交互画面，在这里你可以和大模型进行交互，同时，输出会计算 你的 输入 token 消耗 和 模型的输出token消耗。\n",
    "请注意，`输入消耗` 在 API 消耗的值，是 用户当前输入和对话历史的全部内容，因此，你将会在对话的过程看到 输入使用的 token 逐渐增加。\n",
    "\n",
    "Next, we use a conversation with historical records to calculate the Token consumption rate as the historical records continue to increase.\n",
    "I wrote a Python script, where you can have a continuous conversation with the model, and the code will return the number of Tokens input and the number of Tokens output each time as the conversation proceeds.\n",
    "\n",
    "This code simulates a simple command line interaction screen, where you can interact with the large model. At the same time, the output will calculate your input token consumption and the model's output token consumption.\n",
    "\n",
    "Please note that the value consumed by `input consumption` in the API is the entire content of the user's current input and conversation history. Therefore, you will see the token used for input gradually increase during the conversation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187bed9b6a2e5caa"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, max_tokens=8192):\n",
    "        self.client = ZhipuAI()\n",
    "        self.history = []\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"zai-org/GLM-4.5\")\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def calculate_tokens(self, inputs):\n",
    "        return len(tokenizer.apply_chat_template(\n",
    "            inputs,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True)\n",
    "        )\n",
    "\n",
    "    def chat(self):\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "            self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            print(f\"Human: {user_input}\")\n",
    "            user_tokens = self.calculate_tokens(self.history)\n",
    "            print(\"======================\")\n",
    "            print(f\"Human message with history tokens: {user_tokens}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"glm-4\",\n",
    "                messages=self.history,\n",
    "                top_p=0.7,\n",
    "                temperature=0.9,\n",
    "                stream=False,\n",
    "                max_tokens=self.max_tokens,\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content\n",
    "            print(f\"AI: {reply}\")\n",
    "            self.history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            reply_tokens = self.calculate_tokens(self.history[-2:])\n",
    "            print(\"======================\")\n",
    "            print(f\"AI response tokens: {reply_tokens}\", end=\"\\n**********************\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.643431Z",
     "start_time": "2025-09-01T13:19:49.632011Z"
    }
   },
   "id": "f03d04d64937fc37",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "开始对话吧，通过这次对话，你应该会意识到缩短历史记录，并定期清空历史记录的重要性。\n",
    "\n",
    "Start a conversation！That should help you realize the importance of shortening your history and clearing it regularly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92bda16255169230"
  },
  {
   "cell_type": "code",
   "source": [
    "chat_session = ChatSession()\n",
    "chat_session.chat()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:20:04.844148Z",
     "start_time": "2025-09-01T13:19:49.646849Z"
    }
   },
   "id": "a651bbaad4bd2512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 你好\n",
      "======================\n",
      "Human message with history tokens: 6\n",
      "AI: 你好👋！我是人工智能助手智谱清言，可以叫我小智🤖，很高兴见到你，欢迎问我任何问题。\n",
      "======================\n",
      "AI response tokens: 41\n",
      "**********************\n",
      "Human: 我刚才说的是\n",
      "======================\n",
      "Human message with history tokens: 46\n",
      "AI: 你刚才说的是“你好”，这是一句常见的中文问候语。如果你有其他问题或者需要帮助，请随时告诉我。\n",
      "======================\n",
      "AI response tokens: 38\n",
      "**********************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m chat_session \u001B[38;5;241m=\u001B[39m ChatSession()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mchat_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 19\u001B[0m, in \u001B[0;36mChatSession.chat\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mchat\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 19\u001B[0m         user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m user_input\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquit\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m     21\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Code/glm-cookbook/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[0;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/glm-cookbook/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
