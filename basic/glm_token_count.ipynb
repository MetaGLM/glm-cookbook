{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Token Counting in ZhipuAI GLM API\n",
    "\n",
    "**This tutorial is available in English and is attached below the Chinese explanation**\n",
    "\n",
    "æ­¤ä»£ç å°†è®²è¿°åœ¨ GLM æ¨¡å‹ æ˜¯å¦‚ä½•è®¡ç®— Token çš„æ¶ˆè€—çš„ã€‚è¿™å°†èƒ½å¸®ä½ å¯¹åœ¨ API ä½¿ç”¨ä¸­çš„ Token è®¡ç®—äº§ç”Ÿæ›´æ¸…æ™°çš„è®¤è¯†ã€‚\n",
    "\n",
    "This cookbook will describe how Token consumption is calculated in the GLM model. This will help you have a clearer understanding of Token calculations in API usage."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c55dc5a09277b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Tokenizer and count\n",
    "\n",
    "ç”±äºæ²¡æœ‰ä»»ä½•çš„å…¬å¼€çš„ Zhipu AI tokenè®¡ç®—å·¥å…·ï¼Œå› æ­¤ï¼Œæˆ‘ä½¿ç”¨äº† [chatglm3-6b](https://huggingface.co/zai-org/GLM-4.5) è¿™ä¸ªå¼€æºæ¨¡å‹çš„ tokenizerè¿›è¡ŒåŠ è½½ã€‚è¿™ç§è®¡ç®—æ–¹å¼ä»…èƒ½ä½œä¸ºå‚è€ƒï¼Œå°šä¸”ä¸èƒ½è®¤å®šæ˜¯æœ€ç»ˆçš„ API token è®¡ç®—æ–¹å¼ã€‚å…·ä½“çš„è®¡ä»·æ–¹å¼ä»¥å®˜æ–¹æ–‡æ¡£ä¸ºä¸»ã€‚\n",
    "\n",
    "Since there is no public Zhipu AI token calculation tool, I used the tokenizer of this open source model [chatglm3-6b](https://huggingface.co/zai-org/GLM-4.5) to load it. This calculation method can only be used as a reference and cannot be considered as the final API token calculation method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8977deb25864ec9f"
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.292515Z",
     "start_time": "2025-09-01T13:19:48.043898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zai-org/GLM-4.5\", trust_remote_code=True)"
   ],
   "id": "e463ead175234bf7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "æˆ‘ä»¬ä¹¦å†™ä¸€ä¸ªPython ä»£ç ï¼Œåˆ†åˆ«æœ‰ä»¥ä¸‹ä¸¤ä¸ªåŠŸèƒ½\n",
    "\n",
    "1. `count_encode` : å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸º token å¹¶è®¡ç®— token æ¶ˆè€—ã€‚\n",
    "2. `decode`: è¾“å…¥ä¸€ä¸² token è¿›è¡Œè§£ç ï¼Œå¹¶å±•ç¤º è¯è¯­æ˜¯æ€ä¹ˆè¢«æ‹†å¼€çš„ã€‚\n",
    "\n",
    "we write a Python code with the following two functions:\n",
    "\n",
    "1. `count_encode`: Convert natural language into token and calculate token consumption.\n",
    "2. `decode`: Input a string of tokens to decode and show how the words are split."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7125d5fca775b004"
  },
  {
   "cell_type": "code",
   "source": [
    "def count_encode(inputs: str = \"\"):\n",
    "    encoded_input = tokenizer.encode(inputs)\n",
    "    num_tokens = len(encoded_input)\n",
    "    return encoded_input, num_tokens\n",
    "\n",
    "\n",
    "def decode(inputs: list = []):\n",
    "    decode_sentence = tokenizer.convert_ids_to_tokens(inputs)\n",
    "    return decode_sentence\n",
    "\n",
    "\n",
    "text = \"here is a text demo\"\n",
    "encoded_input, num_tokens = count_encode(text)\n",
    "print(f\"Count Token: {num_tokens}\")\n",
    "print(f\"Token IDs: {encoded_input}\")\n",
    "decode_sentence = decode(encoded_input)\n",
    "print(f\"Decode sentence: {decode_sentence}\", end=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.300417Z",
     "start_time": "2025-09-01T13:19:49.296460Z"
    }
   },
   "id": "5b7ed321306f5849",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token: 5\n",
      "Token IDs: [6739, 374, 264, 1467, 16644]\n",
      "Decode sentence: ['here', 'Ä is', 'Ä a', 'Ä text', 'Ä demo']\t"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token calculation in conversation\n",
    "\n",
    "åˆšæ‰çš„ä»£ç å¹¶ä¸æ˜¯åœ¨çœŸæ­£å¯¹è¯ä¸­ token è®¡ç®—çš„ä½¿ç”¨é‡ï¼Œ åœ¨å¯¹è¯ä¸­ï¼Œç”±äºå­˜åœ¨æ¨¡å‹çš„ **å¯¹è¯æ¨¡æ¿** å’Œ **èŠå¤©å†å²**ï¼Œå¯¹è¯ä¸­çš„ token è®¡ç®—ä¼šæ˜æ˜¾å˜é«˜ã€‚\n",
    "\n",
    "æˆ‘å…ˆå±•ç¤ºäº†åœ¨å¯¹è¯åœºæ™¯ä¸­ï¼Œä¸€æ¡ message çš„å®é™…å¯¹åº”çš„tokenæ•°é‡ã€‚\n",
    "\n",
    "The code just now is not the usage of token calculation in the real conversation. In the conversation, due to the existence of the model's **conversation template** and **chat history**, the token calculation in the conversation will be significantly higher.\n",
    "\n",
    "I first showed the actual number of tokens corresponding to a message in the conversation scenario."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f78f812a8c1f0ea"
  },
  {
   "cell_type": "code",
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"here is a text demo\"})\n",
    "chat_inputs = tokenizer.apply_chat_template(messages,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            tokenize=True)\n",
    "num_tokens = len(chat_inputs)\n",
    "print(f\"Count Token: {num_tokens}\")\n",
    "print(f\"Token IDs: {chat_inputs}\")\n",
    "model_input_tokens = tokenizer.convert_ids_to_tokens(chat_inputs)\n",
    "print(model_input_tokens, end=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.323615Z",
     "start_time": "2025-09-01T13:19:49.320136Z"
    }
   },
   "id": "af6673d5c30f8841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token: 10\n",
      "Token IDs: [151331, 151333, 151336, 198, 6739, 374, 264, 1467, 16644, 151337]\n",
      "['[gMASK]', '<sop>', '<|user|>', 'ÄŠ', 'here', 'Ä is', 'Ä a', 'Ä text', 'Ä demo', '<|assistant|>']\t"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "å¯ä»¥çœ‹åˆ°ï¼Œç”±äº **å¯¹è¯æ¨¡æ¿** çš„å­˜åœ¨ï¼Œè¿™æ¬¡çš„ token è®¡ç®— å¤šå‡ºäº† 4 ä¸ª token æ•°é‡ï¼Œè¿™ä¸ªæ•°é‡å°±æ˜¯æ¨¡æ¿çš„å ç”¨ã€‚**è¿™éƒ¨åˆ† token ä¼šåœ¨ API è¿›è¡Œè®¡è´¹çš„è¿‡ç¨‹ä¸­è¢«æ‰£é™¤**ã€‚\n",
    "\n",
    "It can be seen that due to the existence of **dialogue template**, the token calculation this time has 4 more tokens, which is the occupation of the template. **This part of the token will be deducted during the API billing process**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d59444531c388c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculation of token consumption in actual conversations\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¼€å§‹æ¨¡æ‹Ÿåœ¨å®é™…çš„å¯¹è¯ä¸­ï¼Œtoken æ¶ˆè€—çš„æ–¹å¼ï¼Œé¦–å…ˆï¼Œä½ éœ€è¦å¡«å†™ API \n",
    "Now, we start to simulate the way token is consumed in an actual conversation. First, you need to fill in the API"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2b9d929eee22dfd"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = \"your api key\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.473199Z",
     "start_time": "2025-09-01T13:19:49.465494Z"
    }
   },
   "id": "1293b5a0561a0cfb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€æ®µå¸¦æœ‰å†å²è®°å½•çš„å¯¹è¯æ¥è®¡ç®—éšç€ å†å²è®°å½•çš„ä¸æ–­å¢åŠ ï¼Œ Tokençš„æ¶ˆè€—é€Ÿåº¦ã€‚\n",
    "æˆ‘ä¹¦å†™äº†ä¸€æ®µPythonè„šæœ¬ï¼Œåœ¨è¿™é‡Œï¼Œä½ å¯ä»¥å’Œæ¨¡å‹è¿›è¡Œè¿ç»­å¯¹è¯ï¼Œä»£ç å°†ä¼šè¿”å›éšç€å¯¹è¯çš„è¿›è¡Œï¼Œæ¯æ¬¡è¾“å…¥çš„ Token æ•°é‡ å’Œ è¾“å‡ºçš„ Tokenæ•°é‡ã€‚\n",
    "è¿™ä¸ªä»£ç æ¨¡æ‹Ÿäº†ä¸€ä¸ªç®€å•çš„å‘½ä»¤è¡Œäº¤äº’ç”»é¢ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥å’Œå¤§æ¨¡å‹è¿›è¡Œäº¤äº’ï¼ŒåŒæ—¶ï¼Œè¾“å‡ºä¼šè®¡ç®— ä½ çš„ è¾“å…¥ token æ¶ˆè€— å’Œ æ¨¡å‹çš„è¾“å‡ºtokenæ¶ˆè€—ã€‚\n",
    "è¯·æ³¨æ„ï¼Œ`è¾“å…¥æ¶ˆè€—` åœ¨ API æ¶ˆè€—çš„å€¼ï¼Œæ˜¯ ç”¨æˆ·å½“å‰è¾“å…¥å’Œå¯¹è¯å†å²çš„å…¨éƒ¨å†…å®¹ï¼Œå› æ­¤ï¼Œä½ å°†ä¼šåœ¨å¯¹è¯çš„è¿‡ç¨‹çœ‹åˆ° è¾“å…¥ä½¿ç”¨çš„ token é€æ¸å¢åŠ ã€‚\n",
    "\n",
    "Next, we use a conversation with historical records to calculate the Token consumption rate as the historical records continue to increase.\n",
    "I wrote a Python script, where you can have a continuous conversation with the model, and the code will return the number of Tokens input and the number of Tokens output each time as the conversation proceeds.\n",
    "\n",
    "This code simulates a simple command line interaction screen, where you can interact with the large model. At the same time, the output will calculate your input token consumption and the model's output token consumption.\n",
    "\n",
    "Please note that the value consumed by `input consumption` in the API is the entire content of the user's current input and conversation history. Therefore, you will see the token used for input gradually increase during the conversation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187bed9b6a2e5caa"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, max_tokens=8192):\n",
    "        self.client = ZhipuAI()\n",
    "        self.history = []\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"zai-org/GLM-4.5\")\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def calculate_tokens(self, inputs):\n",
    "        return len(tokenizer.apply_chat_template(\n",
    "            inputs,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True)\n",
    "        )\n",
    "\n",
    "    def chat(self):\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "            self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            print(f\"Human: {user_input}\")\n",
    "            user_tokens = self.calculate_tokens(self.history)\n",
    "            print(\"======================\")\n",
    "            print(f\"Human message with history tokens: {user_tokens}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"glm-4\",\n",
    "                messages=self.history,\n",
    "                top_p=0.7,\n",
    "                temperature=0.9,\n",
    "                stream=False,\n",
    "                max_tokens=self.max_tokens,\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content\n",
    "            print(f\"AI: {reply}\")\n",
    "            self.history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            reply_tokens = self.calculate_tokens(self.history[-2:])\n",
    "            print(\"======================\")\n",
    "            print(f\"AI response tokens: {reply_tokens}\", end=\"\\n**********************\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:19:49.643431Z",
     "start_time": "2025-09-01T13:19:49.632011Z"
    }
   },
   "id": "f03d04d64937fc37",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "å¼€å§‹å¯¹è¯å§ï¼Œé€šè¿‡è¿™æ¬¡å¯¹è¯ï¼Œä½ åº”è¯¥ä¼šæ„è¯†åˆ°ç¼©çŸ­å†å²è®°å½•ï¼Œå¹¶å®šæœŸæ¸…ç©ºå†å²è®°å½•çš„é‡è¦æ€§ã€‚\n",
    "\n",
    "Start a conversationï¼That should help you realize the importance of shortening your history and clearing it regularly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92bda16255169230"
  },
  {
   "cell_type": "code",
   "source": [
    "chat_session = ChatSession()\n",
    "chat_session.chat()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-01T13:20:04.844148Z",
     "start_time": "2025-09-01T13:19:49.646849Z"
    }
   },
   "id": "a651bbaad4bd2512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: ä½ å¥½\n",
      "======================\n",
      "Human message with history tokens: 6\n",
      "AI: ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n",
      "======================\n",
      "AI response tokens: 41\n",
      "**********************\n",
      "Human: æˆ‘åˆšæ‰è¯´çš„æ˜¯\n",
      "======================\n",
      "Human message with history tokens: 46\n",
      "AI: ä½ åˆšæ‰è¯´çš„æ˜¯â€œä½ å¥½â€ï¼Œè¿™æ˜¯ä¸€å¥å¸¸è§çš„ä¸­æ–‡é—®å€™è¯­ã€‚å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜æˆ–è€…éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚\n",
      "======================\n",
      "AI response tokens: 38\n",
      "**********************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m chat_session \u001B[38;5;241m=\u001B[39m ChatSession()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mchat_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 19\u001B[0m, in \u001B[0;36mChatSession.chat\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mchat\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 19\u001B[0m         user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m user_input\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquit\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m     21\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Code/glm-cookbook/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[0;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/glm-cookbook/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
